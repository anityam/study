{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"root","n":1},"1":{"v":"This is learning journal. The intend is to document concepts.\n`---`.\n","n":0.316}}},{"i":2,"$":{"0":{"v":"Kubernetes","n":1},"1":{"v":"# Kubernetes \nThe major orchestration tool in the market that every one uses. \n\n## Introduction\n\n\n## Component\nAll components in kubernetes are loosely coupled actors which communicate through REST API which is backed by a highly available store called etcd.\n![ kubernetes component](./assets/images/k8s-arch.png. \n\nAll the component and the kubelet work in a continuous loop to bring the resources to the current state defined in the etcd and informed by the API server. The api implements push bashed notification system called `watch`(Need more details on this)\n\n### Etcd\n\n\n\n\n## Reading \nhttps://www.mgasch.com/2021/01/listwatch-part-1/","n":0.108}}},{"i":3,"$":{"0":{"v":"gitops","n":1},"1":{"v":"# GitOps\n\nThe core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment match the described state in the repository. If you want to deploy a new application or update an existing one, you only need to update the repository - the automated process handles everything else. Itâ€™s like having cruise control for managing your applications in production.\n\n## Why to use GitOps\n1. Deploy faster and more often: This is the benefit of using version control. \n2. Easy and fast recovery: Easy to move back to previously known configuration\n3. Secure: Let tools apply changes to cluster not the developers directly\n4. Documenting: Easy to watch the git repo for what changes have happen and what are upcoming changes\n\n## How it works\nCentral repository that holds the environment and application setup. It can be branched out into two separate repository or a single git repository. \n\n## Deployment Style\n### Pull Based Deployment\nIn this system there is an operator making sure to sync the environment. So a developer makes changes to the application and new images is build. The operator constantly monitors the image or the environment and will update to the latest code when it sees the new image.\n\n### Push Based Deployment\nThis is a deployment style where a push of code to the application triggers a series of deployment procedures which finally results in environment change in the cluster.\n\n\nReading:\n[GitOps pdf](./gitops.pdf)\n","n":0.064}}},{"i":4,"$":{"0":{"v":"flux","n":1},"1":{"v":"\n# Flux\nFlux is a tool for keeping Kubernetes clusters in sync with sources of configuration (like Git repositories), and automating updates to configuration when there is new code to deploy.\n\n## How it works \n\nIt syncs the Git based kubernetes declarative code with the kubernetes cluster. It deploys operators in cluster who constantly monitor the versioned controlled repository.\n\n\n## GitOps toolkit\n\n### Source\nThe main role of the source is to provide an interface for artifact acquisitions. This contains the location of source of truth and a way to connect to it. For example\n```yaml\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m0s\n  ref:\n    branch: main\n  secretRef:\n    name: flux-system\n  url: ssh://git@github.com/anityam/fluxy\n  ```\n  This above declaration contains the definition of a [git repository](https://github.com/anityam/fluxy) over ssh which is synced at an interval of 1m(defaults to 5m).\n  \n  The source produces an artifact that is consumed by other flux components to perform actions.All sources are specified as Custom Resources in a Kubernetes cluster, examples of sources are GitRepository, OCIRepository, HelmRepository and Bucket resources. More reading on [source resource](https://fluxcd.io/flux/components/source/) and [spec](https://github.com/fluxcd/source-controller/tree/main/docs/spec)\n\n  Features:\n    1. Validate source definitions\n    2. Authenticate to sources (SSH, user/password, API token)\n    3. Validate source authenticity (PGP)\n    4. Detect source changes based on update policies (semver)\n    5. Fetch resources on-demand and on-a-schedule\n    6. Package the fetched resources into a well-known format (tar.gz, yaml)\n    7. Make the artifacts addressable by their source identifier (sha, version, ts)\n    8. Make the artifacts available in-cluster to interested 3rd parties\n    9. Notify interested 3rd parties of source changes and availability (status conditions, events, hooks)\n\n\n## Reconciliation\nThe process to make sure a given state matches a desired state which is declared in a versioned controlled repo.\n\n1. **HelmRelease reconciliation**: ensures the state of the Helm release matches what is defined in the resource, performs a release if this is not the case (including revision changes of a HelmChart resource).\n2. **Bucket reconciliation**: downloads and archives the contents of the declared bucket on a given interval and stores this as an artifact, records the observed revision of the artifact and the artifact itself in the status of resource.\n3. **Kustomization reconciliation**: ensures the state of the application deployed on a cluster matches the resources defined in a Git or OCI repository or S3 bucket.\n\n## Kustomization\nKustomization CRD is local set of overlays that flux needs to reconcile to the cluster. Reconcile runs every 5m but this can be adjusted. If you make any changes to the cluster using kubectl edit/patch/delete, they will be promptly reverted. You either suspend the reconciliation or push your changes to a Git repository.\n\nFor more information, take a look at the [Kustomize FAQ](https://fluxcd.io/flux/faq/#kustomize-questions) and the [Kustomization CRD](https://fluxcd.io/flux/components/kustomize/kustomizations/).\n\n## Bootstrap\nBootstrapping flux to a cluster i.e installing flux component in gitOps manner. Flux manages itself through git similar to other application and infrastructure management. The manifests are applied to the cluster, a GitRepository and Kustomization are created for the Flux components, then the manifests are pushed to an existing Git repository (or a new one is created).  \n\nFor more information, take a look at the [bootstrap documentation](https://fluxcd.io/flux/installation/bootstrap/).\n\n\n## Demo\n[Fluxy](https://github.com/anityam/fluxy)\n\n\n## Reading\n[Documentation](https://fluxcd.io/flux/)\n","n":0.045}}},{"i":5,"$":{"0":{"v":"tools","n":1},"1":{"v":"# Kubernetes Tools\nThis section is a subset of Containers tools where we will entirely discuss upon kubernetes based tools.\n\n## Management\nThe tools that can be used to manage the kubernetes cluster itself.\n\n### K9\nTool to manage kubernetes.[[Kubernetes.tools.K9]]\n\n\n## Shell\n### Kubectx\n`brew install kubectx` This will make it easy to change context between different kubernetes cluster defined in your config file. More detail on [installation](https://github.com/ahmetb/kubectx)\nSetting up autocompletion for kubens and kubectx \n```bash\n mkdir ~/.oh-my-zsh/custom/completions\n chmod -R 755 ~/.oh-my-zsh/custom/completions\n```\nGet the zsh autocompletion from the github repo and create a file in the `~/.oh-my-zsh/custom/completions/{_kubectx.zsh,_kubens.zsh}`. Make it executable and check the script for the location of the cli in your environment. then\n```bash\necho \"fpath=($ZSH/custom/completions $fpath)\" >> ~/.zshrc\n```\n If it still doesn't workout after the source of `~/.zshrc` then add `autoload -U compinit && compinit` to your `zshrc`\n\n### Kubens \n`brew isntall kubectx` will install the kubens tools too which is useful in changing the namespace quickly.\n\n### Kube-ps1\nThis is a addons prompt for the zsh or bash\n```brew install kube-ps1```\n\nupdate your `.zshrc` to add \n```\nplugins=(\n  kube-ps1\n)\n\nPROMPT='$(kube_ps1)'$PROMPT # or RPROMPT='$(kube_ps1)'\n```\n\n### Stern\nFor logs \n```\nbrew install stern\n```\n\n### Popeye\nFind vulnerability in k8 cluster\n```\nbrew install derailed/popeye/popeye\n```\n\n### zsh plugins\nKubectl plugin add it to the `~/.zshrc`\n```\nplugins=(\n    ...\n    kubectl\n)```\n\n\n\n\n\n## Document\nhttps://agrimprasad.com/post/supercharge-kubernetes-setup/ ---> Kubernetes setup\nhttps://github.com/ahmetb/kubectx Kubectx Project\nhttps://github.com/stern/stern --> stern project\nhttps://agrimprasad.com/post/supercharge-kubernetes-setup/ \n","n":0.071}}},{"i":6,"$":{"0":{"v":"Kustomize","n":1},"1":{"v":"# Kustomize\nThis is an important tool that works with kubectl and is widely used in the kubernetes world. This is somehow a substitution of Helm or template based structuring of kubernetes resources.\n\n\n## Documentation\nThe documentation for [kustomize is located here](https://kubectl.docs.kubernetes.io/guides/)","n":0.16}}},{"i":7,"$":{"0":{"v":"K9","n":1},"1":{"v":"# K9\nThis is a tool that can be used to manage the kubernetes cluster. The location for more info on the tool itself is located at [github/k9](https://github.com/derailed/k9s).\n\n## Installation\n```brew install derailed/k9s/k9s```\n\n## Experience\nThe tool is really useful to view a lot of items at once in your cluster. It creates a new terminal based app where you will be able to view all the container based on namespace. Kill or log into pods based on your access level in the cluster and also will be able to get logs. This is helpful when doing an overall inspection of the cluster level and looking at logs. This can be used to tail logs and save the logs locally too.","n":0.093}}},{"i":8,"$":{"0":{"v":"Validation","n":1},"1":{"v":"# Validation\n\nWhile working with the declarative language the bigger problem is with validation. The ecosystem of K8 is increasing with ever increasing list of good CRD(custom resource definition). As the declaration gets bigger so does the change of human error. \nWe will look at some validation tools and how they can be used.\n\n## Code\nhttps://github.com/anityam/validation\n\n\n\n## Scenario\nIn this topic we will be evaluating tools to validate istio and fluxCD crd's. \n\n\n\n## Tools\n\n## Requirement\nTo validate kubernetes based native resources like pod, deployment it is easier as the definition of those resources are build in the installation but requires for other resource validation we will need to get the definitions of the crd's declaration for local use or access to a cluster with the CRD installed.\nIn a gitOps approach validation is good to be done locally before committing it to a automation framework like fluxCD. Each custom resource definition should have a resource definition which can be used.\n\n### Kubectl validate\n[kubectl validate](https://github.com/kubernetes-sigs/kubectl-validate) is an open source tool built by ","n":0.078}}},{"i":9,"$":{"0":{"v":"Networking","n":1}}},{"i":10,"$":{"0":{"v":"Kind","n":1},"1":{"v":"# Kind \n\nThis is a local development environment. This is a tool to build a local k8 cluster in your local laptop for testing or development.The tool is maintained by the kubernetes team.\n\n## Usage \nIt is based on the concept of creating a kubernetes cluster with containers. So it does required the presence of docker or any other container runtime in the system be present. `kind` is a local cli tool that can be use to create, delete or interact with the cluster.\n\n\n### Cli\nThe cli can be installed through go\n```bash \ngo install sigs.k8s.io/kind\n```\n\nIn mac it can be installed through brew\n```bash\nbrew install kind\n```\n\n\n\n## Document\nhttps://kind.sigs.k8s.io/ ---> Kind Docs","n":0.097}}},{"i":11,"$":{"0":{"v":"Go Programming","n":0.707},"1":{"v":"# Go Programming In Kubernetes\n\n## Introduction\nKubernetes is a container orchestration tools which uses declarative method. Kubernetes has an API through which we can interact. It is a REST HTTP based API.\n\n## Working\nIn the beginning of the chain, users declare a high level resources like Pods, Deployments etc. Once the resources are declared then the middle level component called controllers are activated to transfer them into low-levels Pods. Then the schedular distributes those resources in the nodes. Now the node agent deploys the Pod in the scheduled cluster.\n\n## Control Plane\nThe control plane is the main brain of the kubernetes. It handles all the complexity of handling the declared resources in the cluster. It's main components are \n1. The API Server - This is the main gateway through which the communication to the cluster is done\n2. The etcd database - This is a database which stores the state of the cluster. Only accessible through API server.\n3. The controller manager - This manages the controllers which transforms the users declared resources to kubernetes low level components. The controller sync with the API to maintain the state of the resources.\n4. Schedular - It maintains the resource distribution in the nodes. It watches the api server for change\n5. Kubelet - Agent in each nodes. It manages workload specified to it's specific node after schedular schedules it. It watches the API server for any change in its node.\n6. Kube-Proxy - Agent to manage network. Runs on all nodes and watches the API server for any change\n\n## API\n##  Cluster\nCreating a cluster for testing API code can be found at [kubernetesGo](https://github.com/anityam/kubernetesGo).\n\n### OpenAPI\nKubernetes uses the openAPI format to specify it REST API HTTP calls. To check","n":0.06}}},{"i":12,"$":{"0":{"v":"client_go","n":1},"1":{"v":"# Kubernetes Client-Go\n\nThe client go is a restful http client.\n\n## Location\nhttps://github.com/kubernetes/client-go is the repo for the go client.\n\n## Friends\n[Kubernetes/api](https://github.com/kubernetes/api/tree/release-1.24) contains the spec or definition of the kubernetes inbuilt kinds or objects.\n[API Machinery](https://github.com/kubernetes/apimachinery/tree/release-1.24) component that can be used as a building block for kubernetes like api","n":0.149}}},{"i":13,"$":{"0":{"v":"Etcd","n":1},"1":{"v":"# Etcd\nThis is the defacto storage implementation for the kubernetes cluster. Etcd is a separate project which was adopted by kubernetes community to be used as the backend\n\n## Key concepts of Etcd\n\n1. Written in go and has a SDK([clientV3](https://pkg.go.dev/go.etcd.io/etcd/clientv3))\n2. `Watch()` API support which is used by kubernetes also `gRPC` protocols\n3. Support for time-travel queries through Multi-Version Concurrency Control([MVCC](https://en.wikipedia.org/wiki/Multiversion_concurrency_control))\n\n\n## Working\n\n### Key Value store\nIt is like a dictionary {\"key\":\"value\"}. \n\n### Etcd server\n\n\n\n\n\n## More Information\n### Reading\nhttps://pierrezemb.fr/posts/notes-about-etcd/\n\n### Tools\nhttps://hub.docker.com/r/bitnami/etcd/ ---> etcd docker based images\n","n":0.113}}},{"i":14,"$":{"0":{"v":"API","n":1},"1":{"v":"# Kubernetes API\nKubernetes api is the main communication channel for the kubernetes cluster. It is a part of the `kubernetes control plain`. This is the heart of communication\n\n## Function\n1. Serve the kubernetes API for state change\n    1. Reading state : get,list and streaming\n    2. Mutating state: create, update delete\n2. Proxy cluster component so other tools can use them like pod exec by kubectl or log streaming etc\n\n## How \nThe main concept is api calls in kubernetes are `rest API` based calls exposed through https with json or `protobuf` payload. The rest method used are GET,POST,PUT, PATCH, DELETE.\n\n## What \nThe terms used\n1. Kind: All resource types have a concrete representation (their object schema) which is called a kind --> or a simple definition is json-schema of an object\n    1. persistent object like pod, service etc\n    2. list of persistent objects like pods \n    3. Simple --> Special purpose kinds used like APIGroup or APIResource , or simply a specific action on a persistent resource(object)\n2. Group: Collection of related kinds \n3. Version: The version of apiGroup to be used v1alpha1,  v1beta1 etc\n4. Resources: These are the endpoints of the kubernetes api api/{resource}. This can also be termed as an item of a `kind`\n    ```\n    kubectl api-resources\n    kubectl api-resources -v 6 # -v 6 means \"extra verbose logging\"\n    ```\n5. Objects: This is similar to kind which means persistent entities with intent (desired state) and the status (actual state) of the cluster. So it has some defined fields.\n    ```\n    kubectl explain  # commands helps with \n    ```\n\n## Declare\nDeclarative state in kubernetes. `Spec` describes what you want.`Status` is the current state of the resource you described in the `spec`.\n\n## Query\nAPI server can be queried through the REST http calls so once the api server endpoint is exposed we can query the \n`/apis/group/version/{namespace/namespace_name}/resource` --> namespaced resources need the namespace. For more info on the api-resources{endpoint}\n```bash\nkubectl api-resources\nkubectl api-versions\n```\n\n## API Operation\n###  Cluster\nCreating a cluster for testing API code can be found at [kubernetesGo](https://github.com/anityam/kubernetesGo).\n\n### API calls\nIn daily \n\n## Request\nThe request once done is passed through a chain of filters which is processed by [DefaultBuildHandlerChain](https://github.com/kubernetes/kubernetes/blob/2cb31c9333adca3f212920d7a1a4e0a3a239598d/staging/src/k8s.io/apiserver/pkg/server/config.go#L790C6-L790C30). \n\n\n\n## Reading\nhttps://kubernetes.io/docs/reference/using-api/api-concepts/#:~:text=Kubernetes%20API%20terminology&text=All%20resource%20types%20have%20a,also%20usually%20represents%20an%20object --> API\nhttps://iximiuz.com/en/posts/kubernetes-api-structure-and-terminology/ --> Terminologies\n\n## Working repo\nhttps://github.com/anityam/kube-programming \n\n","n":0.053}}},{"i":15,"$":{"0":{"v":"Kind Resource and Groups","n":0.5},"1":{"v":"# Kind, Group and Version in Kubernetes\n## Group\nGroup is a collection of related functionality. As each group progresses over time it is classified into different versions\ne.g. /api/apps/v1 but for the core group it is still /api/v1 like for pods\n\n## Kind\nAPI group-version contains one or more API types, which we call Kinds\n\n## Resource \nIt's the declaration of a specific kind.\n\nSo a particular version of kind is GVK group version kind and a particular resource is GVR group version resource.\n\n## Different API's for Kubernetes\nhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v{what version to look for}","n":0.108}}},{"i":16,"$":{"0":{"v":"API Request","n":0.707},"1":{"v":"# API Request\nKubernetes API is a REST Http API. So basic http calls can be made against the Kubernetes API.\n\n## Setup\n### Kind cluster\nWe will be using a kind cluster to setup the kubernetes cluster and make API calls against the kubernetes control plane created in the Kind cluster\nCreating a cluster for testing API code can be found at [kubernetesGo](https://github.com/anityam/kubernetesGo/blob/main/README.md).\n\n## Request\n\nKubectl is the main API client used for interacting with kubernetes API. Kubectl builds and makes the API calls on behalf of the user but to inspect the calls we can use the verbose option with different levels.\n\n```bash\nkubectl get pods -v6\nI0122 18:26:21.852266   48957 loader.go:395] Config loaded from file:  ~/.kube/config\nI0122 18:26:21.878357   48957 round_trippers.go:553] GET https://127.0.0.1:50107/api?timeout=32s 200 OK in 24 milliseconds\nI0122 18:26:21.881194   48957 round_trippers.go:553] GET https://127.0.0.1:50107/apis?timeout=32s 200 OK in 1 milliseconds\nI0122 18:26:21.890185   48957 round_trippers.go:553] GET https://127.0.0.1:50107/api/v1/namespaces/default/pods?limit=500 200 OK in 2 milliseconds\n```\nThis will print the request header and request url\n\nFor further inDepth information on the request use the `-v8` which shows both request and response\n\n### Labels\nThe most and powerful way of filtering a resource in kubernetes is through `labels`. Similar to \n```bash\nkubectl get po -l k8s-app=kube-proxy -n kube-system\n```\nsimilar to \n```\ncurl http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=k8s-app==kube-proxy&limit=10\n```\n\n#### Advance labels \nAdvance selection can be made with `in` `not in` selectors\n\nin option\n```bash\ncurl http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=k8s-app+notin+(kube-proxy,kindset)&limit=10\n```\nnot in option\n```bash\ncurl http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=k8s-app+in+(kube-proxy,kindset)&limit=10\n```\n\n### Field Selector\nYou can filter resources using a limited set of fields. For all resources, you can filter on the metadata.name field; and for all namespaced resources, you can filter on the metadata.namespace field. Plus some other additional fields are provided\n\n```bash\ncurl http://localhost:8080/api/v1/namespaces/kube-system/pods?fieldSelector=status.phase=Running\n```\n\n### Deleting Resource\n```bash\ncurl -X http://localhost:8080/api/v1/namespaces/project1/pods/nginx\n```\n\n### Updating Resource\nFor updating we need to use the PUT call\n","n":0.062}}},{"i":17,"$":{"0":{"v":"Programming","n":1}}},{"i":18,"$":{"0":{"v":"Go","n":1}}},{"i":19,"$":{"0":{"v":"LearningGo","n":1}}},{"i":20,"$":{"0":{"v":"Chapter2","n":1},"1":{"v":"# Types and Declarations\n\n## Zero value\nAll variables are initialized as zero value in golang if the value is not specified\n\n```\nvar testingValue init\nvar randomArrayValue [3]int\nvar randomSliceValue []int\n```\ntestingValue variables value is initialized as 0, randomArrayValue as [0,0,0] and randomSliceValue as [0,0,0...]","n":0.16}}},{"i":21,"$":{"0":{"v":"Chapter1","n":1},"1":{"v":"# Chapter 1\n## Installation\nGo installation for mac or windows can be done through package installer like `brew` or `chocolatey`.\n```\nbrew install go\n```\n```\nchoco install golang\n```\nValidation of the installation when the executable is installed in the path can be done through\n```\ngo version\n```\n\n## GO Tooling\n### build\nCompiles and builds executable of the code \n```\ngo build\ngo build -o nameToOutput\n```\n\n## Fmt\nFormats the go code \n```\ngo fmt\n```\n\n## Mod\nDependency manager for go module.\n\n## Vet\nVet checks if code is syntactically correct\n\n","n":0.119}}},{"i":22,"$":{"0":{"v":"Container","n":1},"1":{"v":"# Container\nShip fast, Ship light and once environment every where.\n\n","n":0.316}}},{"i":23,"$":{"0":{"v":"kubernetes","n":1}}},{"i":24,"$":{"0":{"v":"Service mesh","n":0.707},"1":{"v":"# Service Mesh\nThe way to manage traffic inside the kubernetes cluster. The services in a fast pace microservice type environment where many applications are talking to each other, the requirements for the proxy( connection flows) for each application becomes to huge to manually manage. \nInstead of injecting logic on each service or having many services for an application is cumbersome. Therefore a service mesh is an infrastructure that handles the data plane for all the services and controls the flow of traffic in a cluster. With  a service mesh the logic for traffic flow \ncan be a stand alone application without introducing the network logic in the application design. Istio is a open source service mesh popular in the company standard. \n\n## Istio\n## Installation \nThe installation of istio control plane is done at the `istio-system` namespace. The process for installation can be followed at https://github.com/anityam/istio . The installation will install the control plane at `istio-system`.\n\n## Component\nIstiod --> This is the main component of the control plane. The custom resources that istio uses like `virtualhost` are converted to envoy configs by istiod\n\n## Traffic Entry point\nThe traffic in the istio cluster moves inside the cluster through the **INGRESS** resource.\n### Concept\nIstiod is the controller for the Ingress resource. It looks at the configs and configures the envoy based proxy. This can have a corresponding nodePort or external cloud based loadbalancer that exposes the traffic \n\n#### Envoy Proxy\n\n\n## Concept\nIstio keeps a database of it's managed service and service endpoints in a registry called service registry. The envoy proxy uses this service registry to route the traffic. Istio has a mechanism of service discovery (Pilot) or [ServiceEntry](https://istio.io/latest/docs/reference/config/networking/service-entry/) configuration can be added to add the service to the service Registry.\nEnvoy proxy distributes traffic based on least load model. If there are more than two loadbalancer or host receiving the traffic than envoy routes the traffic to the host that is handling the least amount of traffic\nOutside of the routes istio can also split traffic across instances \n\n## Apis\n1. Virtual Service\n2. Destination Rule\n3. Service Entry\n4. Gateways\n5. Sidecars\n\n### Ingress Gateway\nThe entry point to the cluster. This is deployed in the `istio-system` namespace. This uses an Envoy proxy for the traffic coming from outside the cluster which is usually a loadbalancer(for external cloud based deployment) or a nodeport. We can use `istioctl` to inspect all the traffic flows that are being listed in the ingress gateway envoy filter. The process running inside the ingressgateway is the \n```bash\nk exec -n istio-system deploy/istio-ingressgateway -- ps  \n    PID TTY          TIME CMD\n      1 ?        00:00:04 pilot-agent\n     20 ?        00:00:21 envoy\n     39 ?        00:00:00 ps\n```\nThe `pilot-agent` bootstraps the envoy filter and creates the rules which can be view by\n```bash\nistioctl -n istio-system proxy-config route deploy/istio-ingressgateway\nNAME     VHOST NAME     DOMAINS     MATCH                  VIRTUAL SERVICE\n         backend        *           /stats/prometheus*     \n         backend        *           /healthz/ready*        \n```\n\nThe next chapter is to open up the traffic flow inside the cluster\n\n### Gateway\nGateway are how traffic comes to the cluster and then it can be routed to a virtual service. The gateway opens up a specific port on the ingress-gateway for incoming traffic. once a gateway is deploy \n```bash\nk get gateway\nNAME                AGE\ncoolstore-gateway   63s\n```\n\nNow when we look at the envoy proxy-config we will see the port being opened \n```bash\nistioctl -n istio-system proxy-config route deploy/istio-ingressgateway\nNAME          VHOST NAME         DOMAINS     MATCH                  VIRTUAL SERVICE\nhttp.8080     blackhole:8080     *           /*                     404\n              backend            *           /stats/prometheus*     \n              backend            *           /healthz/ready* \n```\n**blackhole** is due to the absence of any routing to the upstream cluster. Therefore all the traffic is now passed to a blackhole of 404 so when you hit the endpoint `http://127.0.0.1:3000/` (port 3000 is due to the nodePort setup of istio-ingressgateway) we will see a 404\n\n\n### Virtual Service\nThis routes the traffic from to specific services at the backend. It can be used to define routes to various real services based on the rules.In the VirtualService we will define the routes specified for the upstream traffic. Once the virtual service defines the traffic we can see the traffic routes in the proxy\n\n```bash\nistioctl -n istio-system proxy-config route deploy/istio-ingressgateway\nNAME          VHOST NAME                       DOMAINS                     MATCH                  VIRTUAL SERVICE\nhttp.8080     webapp.istioinaction.io:8080     webapp.istioinaction.io     /*                     webapp-vs-from-gw.istioinaction\n              backend                          *                           /stats/prometheus*     \n              backend                          *                           /healthz/ready* \n```\nWith the route set now the webpage can be viewed at `http://webapp.istioinaction.io:3000/` (here the http://weapp.istioinaction.io is pointing to 127.0.0.1 at `/etc/hosts`). \n### Destination Rule \nThis handles traffic to the specific destination. This is behind the virtual service once the traffic is directed to specific host then the destination service can apply the rules to the traffic\n\n\n### Service Entry\nThis is usually used to define a traffic leaving a cluster where a specific call in the cluster can be routed to an entry point pointing to the external cluster.\n\n### sidecars\nTo define a special rules for the sidecars used by the istio\n\n\n\n\n\n\n\n## Document\nhttps://istio.io/latest/docs/concepts/traffic-management/ --> Traffic management read\nhttps://banzaicloud.com/blog/backyards-ingress/  ---> Ingress Istio\nhttps://istio.io/latest/docs/concepts/traffic-management/#virtual-services --> Virtual Service","n":0.036}}},{"i":25,"$":{"0":{"v":"Envoy","n":1},"1":{"v":"# Envoy\n\nEnvoy is the backbone of the Istio service mesh. It is an open source project that lives in https://envoyproxy.io .\n\nIt is a transparent proxy. The main concept of which was established to create a transparent network proxy which is easy to debug. A proxy is a middleman that sits between the client and the server and through which the traffic flows for the communication between server and client.\n Client ---> Proxy ---> Server\n Technically a proxy can stand between the client and server and simplify the setting. The communication logic can be placed in the proxy without the server and client having to delete with the communication logic and able to focus on their specific client or server based logics.\n\n## Concept\n Envoy is designed as an application level proxy and can understand the layer 7 protocols. It can be extended to understand other level proxies. Lot of more telemetries come out of box due to Envoy understanding application level protocols\n\n\n## Feature\nListeners --> ports where downstream(something sending information from to the server or to the )\nRoutes --> The route setup for the downstream routes to be moved to the upstream clusters\nClusters --> the server portion or the \n\n### Service Discovery\nIt uses service discovery api to look for the service. \n\n### Load Balancing\nIt uses local-aware load balancing. Can be configured to use other routing strategies.\n\n### Traffic and Request Routing\nSophisticated routing rules since it can understand L7 protocols like HTTP/1 or HTTp/2\n\n### Traffic Shifting and Shadowing\nEnvoy can split traffic based on weight and also move copy of traffic to certain version( only a copy of the traffic the real flow still can use the other version)\n\n### Network Resilience\nRetries, timeouts, Health checks can be configured in Envoy.\n\n### Metrics\nIt can generate a lot of metrics\n\n### Tracing\nIt will generate x-request-id header to trace traffic.\n\n### Tls and Mutul Tls, Tls termination \n\n### Rate limiting\n\n\n## Documents \n\nhttps://envoyproxy.io ---> the main place for envoy\nhttps://www.envoyproxy.io/docs/envoy/v1.27.0/intro/arch_overview/intro/terminology --> docs","n":0.056}}},{"i":26,"$":{"0":{"v":"tools","n":1},"1":{"v":"# Tools\n\nTools are the basic building blocks of doing any work. So knowing the tools for work is the uptmost important details.\n\n\n","n":0.213}}},{"i":27,"$":{"0":{"v":"Validation","n":1},"1":{"v":"# Validation\n\n## Validating Istio tools\n","n":0.447}}},{"i":28,"$":{"0":{"v":"cluster","n":1},"1":{"v":"\n# Cluster \nThe main orchestration tool for the contianers that will be deployed. The approach is to have a local cluster using a tool like kind or minikube and then deploye and maintain the resources in the cluster using a tool like flux.\n\n## Local Deployment\nThe main initial step for the cluster is to have an infrastrucutre where the cluster can be installed. Just like any other piece of software the cluster orchestration tool requires an infrastructure which is a seriers of servers or contianers that provides an interface like a server.In a enterprise level that can be a datacenter with a series of servers with extensive cpus, memory and storage or a cloud based interface like eks, ecs etc. For a local deployment we will be using the virtualization tools like hyperV to build a virutal servers or container based environment with kind or minikube.\n\nIn this example we will use KinD.\n\n","n":0.081}}},{"i":29,"$":{"0":{"v":"tools","n":1},"1":{"v":"\n# Tools\n\n## KinD\nBasically stands for Kubernetes in Docker. As the name suggests this is a tool to instantiate a kubernetes cluster using docker containers in a local system. The location for this project is at [github/KinD](https://github.com/kubernetes-sigs/kind).\n\n### Installation\nKinD can be installed using the go programming language ```go install sigs.k8s.io/kind@v0.20.0``` or with brew  ```brew install kind```\n","n":0.136}}}]}
